{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "63f9f478-d475-44ce-8d39-63328e25e546",
   "metadata": {
    "tags": []
   },
   "source": [
    "## statistics_PSD_data.ipynb\n",
    "\n",
    "Reads the hourly PSD data from fr2_PSD_##.nc4\n",
    "* Calculates a number of statistics as a function of hour and wavelength:\n",
    "    * The exp(mean(log)) across cases of PSD for variables in each HET and HOM sets\n",
    "    * exp(mean(log(HET/HOM))) across cases of PSD for variables\n",
    "    * Log variance across cases of PSD for variables in each HET and HOM sets\n",
    "    * Correlation (HOM vs HET) of log(PSD) for specific variables across cases\n",
    "    * The relative entropy (HOM vs HET) across cases of PSD for variables\n",
    "* Writes out fr2_stats2_PSD.nc4\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "edfb05bb-92ef-40b1-b045-def2083f2e82",
   "metadata": {},
   "outputs": [],
   "source": [
    "import xarray as xr\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "from scipy.stats import entropy, spearmanr\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.colors as colors\n",
    "import matplotlib as mpl\n",
    "\n",
    "import sys, warnings, glob\n",
    "warnings.filterwarnings(\"ignore\", category=RuntimeWarning)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b4717122-9012-4f6a-95bf-0980d33ca99b",
   "metadata": {},
   "source": [
    "## The function below performs smoothing on power spectra in frequency space"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "12a05035-d110-4754-a601-33c903d5c5db",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "##############################################################\n",
    "### Function to smooth out ripples in data in frequency space\n",
    "def freq_smooth2(freq,psd,winfac=1):\n",
    "    \"\"\"\n",
    "    Required inputs:\n",
    "    freq   (float 1D array) = frequencies (ascending) [Hz]\n",
    "    psd    (float 1D or 2D array) = corresponding power spectral density [v**2/Hz]\n",
    "    \n",
    "    Optional inputs:\n",
    "    winfac          (float) = scaling factor for window size as function of frequency\n",
    "    \n",
    "    Output:\n",
    "    smooth (float 1D or 2D array) = Smoothed version of psd\n",
    "    \"\"\"\n",
    "    \n",
    "    smooth = psd.copy()\n",
    "    if freq.ndim != 1:\n",
    "        raise IndexError(\"Frequencies must be in a 1D array\")\n",
    "    win = np.rint(np.exp(np.sqrt(freq)*winfac))*2 - 1 # Window width\n",
    "    \n",
    "    if len(psd.shape) == 1:  # 1D case - easy:  \n",
    "        if freq.shape != psd.shape:\n",
    "            raise IndexError(\"Required input series are not the same length\")\n",
    "        for n in range(len(freq)):\n",
    "            t0 = int((win[n]-1)/2)\n",
    "            t1 = np.min((int((win[n]-1)/2)+1,len(freq)))\n",
    "            smooth[n] = psd[n-t0:n+t1].mean()\n",
    "            \n",
    "    elif len(psd.shape) == 2:  # 2D case - harder:\n",
    "        if q_lmean.wavelength.size not in psd.shape:\n",
    "            raise IndexError(\"No PSD array dimension matches frequency series length\")\n",
    "        idx = psd.shape.index(freq.size) # This is the matching dimension for frequencies\n",
    "        for n in range(len(freq)):\n",
    "            t0 = int((win[n]-1)/2)\n",
    "            t1 = np.min((int((win[n]-1)/2)+1,len(freq)))\n",
    "            for j in range(psd.shape[1-idx]):\n",
    "                if idx == 1:\n",
    "                    smooth[:,n] = psd[:,n-t0:n+t1].mean(axis=idx)   \n",
    "                else:\n",
    "                    smooth[n,:] = psd[n-t0:n+t1,:].mean(axis=idx)   \n",
    "        \n",
    "    else:\n",
    "        raise IndexError(\"Required PSD array cannot exceed 2D\")\n",
    "        \n",
    "    return smooth"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "96dd51b4-02f0-4828-997b-61cccb573200",
   "metadata": {},
   "source": [
    "### Some large arrays and strings are set below to be used to annotate the dataset produced "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3cf56f6d-8aed-45dd-b851-6517e6188c63",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Approximate pressures [Pa] corresponding to model levels (just a time average from one of the cases)\n",
    "p_levs = np.array([97741.44 , 97398.18 , 97057.56 , 96719.125, 96382.766, 96048.21 ,\n",
    "       95715.01 , 95382.93 , 95051.836, 94721.61 , 94392.29 , 94063.93 ,\n",
    "       93736.61 , 93410.234, 93084.7  , 92760.07 , 92436.27 , 92113.336,\n",
    "       91791.305, 91470.1  , 91149.69 , 90830.11 , 90511.375, 90193.49 ,\n",
    "       89876.516, 89560.45 , 89245.33 , 88931.15 , 88617.95 , 88305.68 ,\n",
    "       87994.25 , 87683.74 , 87374.19 , 87065.664, 86758.15 , 86451.63 ,\n",
    "       86146.15 , 85841.67 , 85538.15 , 85235.555, 84933.87 , 84633.02 ,\n",
    "       84332.984, 84033.75 , 83735.375, 83437.89 , 83141.336, 82845.664,\n",
    "       82550.92 , 82257.12 , 81964.19 , 81672.19 , 81381.08 , 81090.89 ,\n",
    "       80801.55 , 80513.   , 80225.305, 79938.43 , 79652.38 , 79367.18 ,\n",
    "       79082.86 , 78799.44 , 78516.85 , 78235.08 , 77954.195, 77674.195,\n",
    "       77395.02 , 77116.67 , 76839.12 , 76562.3  , 76286.25 , 76010.984,\n",
    "       75736.52 , 75462.9  , 75190.05 , 74917.96 , 74646.664, 74376.14 ,\n",
    "       74106.49 , 73837.71 , 73569.74 , 73302.67 , 73036.484, 72771.305,\n",
    "       72507.17 , 72244.055, 71981.96 , 71720.85 , 71460.63 , 71201.58 ,\n",
    "       70943.81 , 70687.1  , 70431.24 , 70176.24 , 69922.11 , 69669.07 ,\n",
    "       69417.2  , 69166.414, 68916.56 , 68667.766, 68419.945, 68172.92 ,\n",
    "       67926.734, 67681.36 , 67436.77 , 67192.9  , 66949.695, 66707.21 ,\n",
    "       66465.414, 66224.28 , 65983.88 , 65744.23 , 65505.37 , 65267.23 ,\n",
    "       65029.78 , 64793.008, 64556.918, 64321.508, 64086.785, 63852.684,\n",
    "       63619.258, 63386.53 , 63154.508, 62923.2  , 62692.56 , 62462.605,\n",
    "       62233.395, 62004.88 , 61777.07 , 61549.95 , 61323.496, 61097.684,\n",
    "       60872.566, 60648.1  , 60424.316, 60201.258, 59978.914, 59757.258,\n",
    "       59536.215, 59315.805, 59095.945, 58876.695, 58658.   , 58439.887,\n",
    "       58222.355, 58005.477, 57789.383, 57574.125, 57359.594, 57145.78 ,\n",
    "       56932.754, 56720.55 , 56509.07 , 56298.285, 56088.215, 55878.82 ,\n",
    "       55670.117, 55462.133, 55254.883, 55048.38 , 54842.582, 54637.402,\n",
    "       54432.78 , 54228.758, 54025.383, 53822.62 , 53620.414, 53413.402,\n",
    "       53195.96 , 52967.26 , 52727.133, 52475.32 , 52210.676, 51933.094,\n",
    "       51642.125, 51336.582, 51016.254, 50680.44 , 50328.12 , 49958.906,\n",
    "       49571.812, 49166.34 , 48741.938, 48297.59 , 47833.152, 47347.863,\n",
    "       46840.688, 46310.96 , 45757.793, 45180.402, 44578.145, 43949.965,\n",
    "       43294.848, 42612.16 , 41901.027, 41161.133, 40391.66 , 39591.812,\n",
    "       38761.625, 37902.418, 37015.66 , 36099.28 , 35151.277, 34172.605,\n",
    "       33164.355, 32126.564, 31060.488, 29966.203, 28843.734, 27694.73 ,\n",
    "       26527.67 , 25374.379, 24258.316, 23178.521, 22135.934, 21129.643,\n",
    "       20157.207, 19218.822, 18314.855, 17444.225, 16605.197, 15797.964,\n",
    "       15021.669, 14274.116, 13556.278, 13073.933])\n",
    "# Approximate heights [m AGL] corresponding to model levels (just a time average from one of the cases)\n",
    "z_levs = np.array([   15.697305,    47.003498,    78.152885,   109.18822 ,\n",
    "         140.12819 ,   170.99742 ,   201.83646 ,   232.66306 ,\n",
    "         263.48798 ,   294.31656 ,   325.14484 ,   355.9643  ,\n",
    "         386.77017 ,   417.56982 ,   448.3696  ,   479.1682  ,\n",
    "         509.96713 ,   540.7652  ,   571.5618  ,   602.35944 ,\n",
    "         633.1591  ,   663.9615  ,   694.76434 ,   725.56433 ,\n",
    "         756.3584  ,   787.1455  ,   817.9245  ,   848.6918  ,\n",
    "         879.4479  ,   910.1955  ,   940.9394  ,   971.6786  ,\n",
    "        1002.4047  ,  1033.1143  ,  1063.8088  ,  1094.4874  ,\n",
    "        1125.1477  ,  1155.7919  ,  1186.4244  ,  1217.0476  ,\n",
    "        1247.6613  ,  1278.2714  ,  1308.8835  ,  1339.496   ,\n",
    "        1370.1034  ,  1400.703   ,  1431.293   ,  1461.8727  ,\n",
    "        1492.4412  ,  1522.9967  ,  1553.543   ,  1584.0798  ,\n",
    "        1614.6058  ,  1645.1223  ,  1675.6332  ,  1706.143   ,\n",
    "        1736.652   ,  1767.1573  ,  1797.6582  ,  1828.1549  ,\n",
    "        1858.6423  ,  1889.1199  ,  1919.5938  ,  1950.0632  ,\n",
    "        1980.5247  ,  2010.9766  ,  2041.4221  ,  2071.8645  ,\n",
    "        2102.3044  ,  2132.7468  ,  2163.1902  ,  2193.6335  ,\n",
    "        2224.0728  ,  2254.5073  ,  2284.9392  ,  2315.3723  ,\n",
    "        2345.806   ,  2376.2366  ,  2406.659   ,  2437.0747  ,\n",
    "        2467.486   ,  2497.892   ,  2528.2888  ,  2558.6653  ,\n",
    "        2589.0173  ,  2619.3489  ,  2649.6565  ,  2679.9517  ,\n",
    "        2710.24    ,  2740.493   ,  2770.6968  ,  2800.8716  ,\n",
    "        2831.0376  ,  2861.2036  ,  2891.366   ,  2921.5012  ,\n",
    "        2951.5933  ,  2981.662   ,  3011.7202  ,  3041.7522  ,\n",
    "        3071.766   ,  3101.7798  ,  3131.7915  ,  3161.7935  ,\n",
    "        3191.7917  ,  3221.795   ,  3251.802   ,  3281.812   ,\n",
    "        3311.8276  ,  3341.8484  ,  3371.869   ,  3401.884   ,\n",
    "        3431.8918  ,  3461.895   ,  3491.8982  ,  3521.9026  ,\n",
    "        3551.9094  ,  3581.9119  ,  3611.916   ,  3641.9243  ,\n",
    "        3671.9336  ,  3701.9375  ,  3731.9363  ,  3761.933   ,\n",
    "        3791.9285  ,  3821.9216  ,  3851.9084  ,  3881.8914  ,\n",
    "        3911.8716  ,  3941.8499  ,  3971.8286  ,  4001.8088  ,\n",
    "        4031.7869  ,  4061.764   ,  4091.7395  ,  4121.7046  ,\n",
    "        4151.6597  ,  4181.609   ,  4211.556   ,  4241.5034  ,\n",
    "        4271.4565  ,  4301.4126  ,  4331.3726  ,  4361.3364  ,\n",
    "        4391.305   ,  4421.273   ,  4451.221   ,  4481.1436  ,\n",
    "        4511.056   ,  4540.9644  ,  4570.859   ,  4600.734   ,\n",
    "        4630.601   ,  4660.466   ,  4690.3296  ,  4720.19    ,\n",
    "        4750.0503  ,  4779.906   ,  4809.75    ,  4839.5845  ,\n",
    "        4869.413   ,  4899.247   ,  4929.0938  ,  4958.9478  ,\n",
    "        4988.8003  ,  5018.6523  ,  5048.5146  ,  5079.1836  ,\n",
    "        5111.4976  ,  5145.5986  ,  5181.529   ,  5219.3506  ,\n",
    "        5259.2583  ,  5301.2915  ,  5345.5483  ,  5392.237   ,\n",
    "        5441.4194  ,  5493.2476  ,  5547.9185  ,  5605.5356  ,\n",
    "        5666.307   ,  5730.3726  ,  5797.89    ,  5869.096   ,\n",
    "        5944.101   ,  6023.112   ,  6106.4023  ,  6194.1836  ,\n",
    "        6286.7466  ,  6384.3604  ,  6487.284   ,  6595.8438  ,\n",
    "        6710.4106  ,  6831.275   ,  6958.7915  ,  7093.227   ,\n",
    "        7234.981   ,  7384.4844  ,  7542.1406  ,  7708.2354  ,\n",
    "        7882.8984  ,  8066.8276  ,  8260.93    ,  8465.5625  ,\n",
    "        8681.12    ,  8908.353   ,  9147.682   ,  9399.853   ,\n",
    "        9665.779   ,  9946.119   , 10239.759   , 10539.291   ,\n",
    "       10838.875   , 11138.76    , 11438.572   , 11738.272   ,\n",
    "       12038.477   , 12339.073   , 12639.703   , 12940.302   ,\n",
    "       13241.187   , 13542.021   , 13842.673   , 14143.909   ,\n",
    "       14445.741   , 14724.665   ])\n",
    "# Long description strings for the dataset:\n",
    "d_string = \"\"\"\n",
    "Data are from LES simulations with heterogeneous (HET) and homogeneous (HOM) surface states and fluxes \n",
    "   • Lower boundary prescribed from HydroBlocks offline simulations\n",
    "   • PSD for variables here are calulated hourly in horizontal space (directionless) across LES domain\n",
    "     • 92 case days, each with 14 hours\n",
    "     • 259 wavenumbers (domain 520x520)\n",
    "   • For 3D variables, vertical coordinates are approximate, to align with model levels - makes plots easier to interpret\n",
    "     \n",
    "All statistics are calcualted each hour as a function of wavelength: \n",
    "   • No data at t=0 (hour 12UTC) as the ICs are the same\n",
    "   • Data for 14 hours (13 to 03UTC) are saved\n",
    "   • Across wavelengths; results can be plotted in spectral space\n",
    "   • 3D fields also have a vertical coordinate to their statistics\n",
    "\"\"\"\n",
    "s_string = \"\"\"\n",
    "Means are calculated as the exponent of the mean of the logs of power spectral density (PSD) across all 92 cases in HET and HOM.\n",
    "   • That is: exp(mean(log(PSD)))\n",
    "   • These are used to normalize the case values to produce a sort of anomaly\n",
    "     • Normalized values (0,1) have less power than \"mean\"\n",
    "     • Normalized values >1 have more power than \"mean\"\n",
    "     \n",
    "Similar to the log-mean, a \"log-variance\" is calculated across all 92 cases for HET and HOM:\n",
    "   • Equals the mean of the normalized PSDs for each case \n",
    "   • No variance from case to case would result in a value of 1\n",
    "   • More case-to-case variance leads to values ever greater than 1\n",
    "   \n",
    "An exp-mean-log-ratio between HET and HOM across cases:\n",
    "   • That is: exp(mean(log(ratio))) applied to non-normalized PSDs\n",
    "   • Works like RMS difference, but in log space\n",
    "   • HET is in the numerator, so expect values >1 mainly - i.e., mismatches driven by surface heterogeneity leads to bigger numbers\n",
    "\n",
    "Pearson's correlation between HET and HOM across cases (applied to the log of PSDs):\n",
    "   • High correlations suggest the area-mean surface ICs or synoptic atmospheric situation controls PSD\n",
    "   • Low correlation suggests the surfae heterogeneity affects the power spectrum\n",
    "\n",
    "Relative entropy (RE) is calculated between HET (baseline, q) and HOM (p) cases.\n",
    "   • PSDs are normalized by exp(mean(log(PSD))) to put all wavenumbers on a more equal footing\n",
    "   • 8 bins for PSD, each spans a power of 10 \n",
    "     • count number of cases in each bin\n",
    "     • 1 is added to each bin to avoid div0 when any q bins are empty\n",
    "     • Thus, reads like 100 (92 + 8) count across all bins\n",
    "     • Then converted to a probability distribution (Σ=1.0)\n",
    "   • RE is calculated as function of wavelength, (and level for 3D fields)\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f87b384b-0975-4c38-99cf-80a6267aeaa7",
   "metadata": {},
   "outputs": [],
   "source": [
    "#################################\n",
    "# Open files\n",
    "\n",
    "ddir = \"/Volumes/SSD_8TB/CLASP/LES_runs2/\"  # Part to the output from `spatial_PSD_data.ipynb` \n",
    "cases = ['_00','_01']\n",
    "\n",
    "# Open files\n",
    "het = xr.open_dataset(f\"{ddir}fr2_PSD{cases[0]}.nc4\")\n",
    "hom = xr.open_dataset(f\"{ddir}fr2_PSD{cases[1]}.nc4\")\n",
    "\n",
    "het"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "af73eeab-530f-4ab4-baf6-4d852b97b874",
   "metadata": {},
   "source": [
    "## Calculate statistics between HOM and HET fields and write to file\n",
    "Includes:\n",
    "* Variance of power, and log(power), across cases for each config\n",
    "* EMDL (exponent mean difference of logs, an application of log power averaging) between configs\n",
    "* Spearman ranked correlation between configs\n",
    "* Relative entropy "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e0f0ad4d",
   "metadata": {},
   "outputs": [],
   "source": [
    "######################################################\n",
    "# Create xarrays of statistics with these dimensions:\n",
    "#   • wavelength\n",
    "#   • [level]\n",
    "#   • hour \n",
    "#\n",
    "# RE is between HET (baseline, q) and HOM (p) cases\n",
    "#   • Applied to power spectral density in horizontal space (directionless) across LES domain\n",
    "#     • 92 case days, each with 14 hours\n",
    "#     • 259 wavenumbers (domain 520x520)\n",
    "#   • PSD each hour is normalized by exp(mean(log(PSDs))) averaged across all cases\n",
    "#     to put all wavenumbers on a more equal footing\n",
    "#   • 8 bins for power, each spans factor of 10 \n",
    "#     • count number of cases in each bin\n",
    "#     • 1 added to each bin to avoid div0 when q bins are empty\n",
    "#     • Thus, reads like 100 count across all bins\n",
    "#     • Then converted to a probability distribution (Σ=1.0)\n",
    "#   • RE calculated as function of wavelength, (and level for 3D fields)\n",
    "#     • smoothed across frequency space to remove noise from smal sample\n",
    "\n",
    "winfac = 4.5 # factor for smoothing spectra\n",
    "bins = [1e-20,1e-3,1e-2,1e-1,1e0,1e1,1e2,1e3,1e20] # for a normalized distribution\n",
    "bin_names = [\"<1e-3\",\"1e-3:1e-2\",\"1e-2:1e-1\",\"1e-1:1e0\",\"1e0:1e1\",\"1e1:1e2\",\"1e2:1e3\",\">1e3\"] \n",
    "\n",
    "first = True\n",
    "###>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>\n",
    "for v in het.data_vars:  # Loop thru variables    \n",
    "    print(f\"\\033[1m{v}\\033[0m\",end=\": \")\n",
    "\n",
    "    sam_p,sam_q = hom[v],het[v] # Choose the variables\n",
    "\n",
    "    p_lmean = np.exp(np.log(sam_p).groupby('time.hour').mean()) # exp of mean of logs\n",
    "    p_lmean = xr.concat([p_lmean[3:],p_lmean[:3]],dim='hour')   # In proper order by hour\n",
    "    q_lmean = np.exp(np.log(sam_q).groupby('time.hour').mean()) # exp of mean of logs, use for normalization\n",
    "    q_lmean = xr.concat([q_lmean[3:],q_lmean[:3]],dim='hour')   # In order again\n",
    "    hours = list(q_lmean.hour.values)\n",
    "\n",
    "    ####### Make some empty data arrays for stats:\n",
    "    # Relative entropy\n",
    "    rex_name = f\"re_{q_lmean.name}\"\n",
    "    rex = xr.zeros_like(q_lmean).rename(\n",
    "          rex_name).assign_coords(\n",
    "          hour = [f\"{h:02}\" for h in hours]).assign_attrs(\n",
    "          {'units':\"-\",'description':f\"relative entropy (HOM vs HET) of {q_lmean.attrs['description']}\"})\n",
    "    rex['wavelength'].attrs = {\"units\": \"grid cells\"}\n",
    "    rex['hour'].attrs = {\"time reference\": \"UTC\"}\n",
    "    # exp-mean-log of the ratio HET/HOM\n",
    "    emlr_name = f\"emlr_{q_lmean.name}\"\n",
    "    emlr = xr.zeros_like(q_lmean).rename(\n",
    "          emlr_name).assign_coords(\n",
    "          hour = [f\"{h:02}\" for h in hours]).assign_attrs(\n",
    "          {'units':\"-\",'description':f\"exp(mean(log(HET/HOM))) of {q_lmean.attrs['description']}\"})\n",
    "    emlr['wavelength'].attrs = {\"units\": \"grid cells\"}\n",
    "    emlr['hour'].attrs = {\"time reference\": \"UTC\"}\n",
    "    # Correlation (Pearson)\n",
    "    corr_name = f\"corr_{q_lmean.name}\"\n",
    "    corr = xr.zeros_like(q_lmean).rename(\n",
    "          corr_name).assign_coords(\n",
    "          hour = [f\"{h:02}\" for h in hours]).assign_attrs(\n",
    "          {'units':\"-\",'description':f\"correlation (HOM vs HET) of {q_lmean.attrs['description']}\"})\n",
    "    corr['wavelength'].attrs = {\"units\": \"grid cells\"}\n",
    "    corr['hour'].attrs = {\"time reference\": \"UTC\"}\n",
    "    # Logs of variances across cases\n",
    "    lv_hom_name = f\"lv_hom_{q_lmean.name}\"\n",
    "    lv_hom = xr.zeros_like(q_lmean).rename(\n",
    "          lv_hom_name).assign_coords(\n",
    "          hour = [f\"{h:02}\" for h in hours]).assign_attrs(\n",
    "          {'units':\"-\",'description':f\"Log variance across HOM cases of {q_lmean.attrs['description']}\"})\n",
    "    lv_hom['wavelength'].attrs = {\"units\": \"grid cells\"}\n",
    "    lv_hom['hour'].attrs = {\"time reference\": \"UTC\"}\n",
    "    lv_het_name = f\"lv_het_{q_lmean.name}\"\n",
    "    lv_het = xr.zeros_like(q_lmean).rename(\n",
    "          lv_het_name).assign_coords(\n",
    "          hour = [f\"{h:02}\" for h in hours]).assign_attrs(\n",
    "          {'units':\"-\",'description':f\"Log variance across HET cases of {q_lmean.attrs['description']}\"})\n",
    "    lv_het['wavelength'].attrs = {\"units\": \"grid cells\"}\n",
    "    lv_het['hour'].attrs = {\"time reference\": \"UTC\"}\n",
    "    \n",
    "    ########################################\n",
    "    # Means (exp(mean(log(X)))) across cases\n",
    "    eml_hom_name = f\"eml_hom_{q_lmean.name}\"\n",
    "    eml_hom = p_lmean.rename(\n",
    "          eml_hom_name).assign_coords(\n",
    "          hour = [f\"{h:02}\" for h in hours]).assign_attrs(\n",
    "          {'description':f\"exp-mean-log across HOM cases of {q_lmean.attrs['description']}\"})\n",
    "    eml_hom['wavelength'].attrs = {\"units\": \"grid cells\"}\n",
    "    eml_hom['hour'].attrs = {\"time reference\": \"UTC\"}\n",
    "    eml_het_name = f\"eml_het_{q_lmean.name}\"\n",
    "    eml_het = q_lmean.rename(\n",
    "          eml_het_name).assign_coords(\n",
    "          hour = [f\"{h:02}\" for h in hours]).assign_attrs(\n",
    "          {'description':f\"exp-mean-log across HET cases of {q_lmean.attrs['description']}\"})\n",
    "    eml_het['wavelength'].attrs = {\"units\": \"grid cells\"}\n",
    "    eml_het['hour'].attrs = {\"time reference\": \"UTC\"}\n",
    "    \n",
    "    if len(sam_p.shape) == 3:  # Add plottable (if approximate) vertical coordinates\n",
    "        rex = rex.assign_coords({\"p_levs\":('bottom_top',p_levs), \"z_levs\":('bottom_top',z_levs)})\n",
    "        emlr = emlr.assign_coords({\"p_levs\":('bottom_top',p_levs), \"z_levs\":('bottom_top',z_levs)})\n",
    "        corr = corr.assign_coords({\"p_levs\":('bottom_top',p_levs), \"z_levs\":('bottom_top',z_levs)})\n",
    "        lv_hom = lv_hom.assign_coords({\"p_levs\":('bottom_top',p_levs), \"z_levs\":('bottom_top',z_levs)})\n",
    "        lv_het = lv_het.assign_coords({\"p_levs\":('bottom_top',p_levs), \"z_levs\":('bottom_top',z_levs)})\n",
    "        eml_hom = eml_hom.assign_coords({\"p_levs\":('bottom_top',p_levs), \"z_levs\":('bottom_top',z_levs)})\n",
    "        eml_het = eml_het.assign_coords({\"p_levs\":('bottom_top',p_levs), \"z_levs\":('bottom_top',z_levs)})\n",
    "        rex['p_levs'].attrs = {\"units\": \"Pa (approx.)\"}\n",
    "        emlr['p_levs'].attrs = {\"units\": \"Pa (approx.)\"}\n",
    "        corr['p_levs'].attrs = {\"units\": \"Pa (approx.)\"}\n",
    "        lv_hom['p_levs'].attrs = {\"units\": \"Pa (approx.)\"}\n",
    "        lv_het['p_levs'].attrs = {\"units\": \"Pa (approx.)\"}\n",
    "        eml_hom['p_levs'].attrs = {\"units\": \"Pa (approx.)\"}\n",
    "        eml_het['p_levs'].attrs = {\"units\": \"Pa (approx.)\"}\n",
    "        rex['z_levs'].attrs = {\"units\": \"m AGL (approx.)\"}\n",
    "        emlr['z_levs'].attrs = {\"units\": \"m AGL (approx.)\"}\n",
    "        corr['z_levs'].attrs = {\"units\": \"m AGL (approx.)\"}\n",
    "        lv_hom['z_levs'].attrs = {\"units\": \"m AGL (approx.)\"}\n",
    "        lv_het['z_levs'].attrs = {\"units\": \"m AGL (approx.)\"}\n",
    "        eml_hom['z_levs'].attrs = {\"units\": \"m AGL (approx.)\"}\n",
    "        eml_het['z_levs'].attrs = {\"units\": \"m AGL (approx.)\"}\n",
    "\n",
    "    # Loop through hours to calculate stats across cases - reassemple into xarrays\n",
    "    for h in hours:\n",
    "        print(h,end=\" \")\n",
    "\n",
    "        # Extract data just for this hour\n",
    "        q_ens = sam_q.where(sam_q.time['time.hour']==h).dropna(dim='time')\n",
    "        p_ens = sam_p.where(sam_p.time['time.hour']==h).dropna(dim='time')\n",
    "        \n",
    "        # The log-variances are straightforward\n",
    "        lv_het.loc[dict(hour=f\"{h:02}\")] = freq_smooth2(1/q_lmean.wavelength,(q_ens/q_lmean.sel(hour=h)).var(dim='time'),winfac=winfac)\n",
    "        lv_hom.loc[dict(hour=f\"{h:02}\")] = freq_smooth2(1/q_lmean.wavelength,(p_ens/p_lmean.sel(hour=h)).var(dim='time'),winfac=winfac)\n",
    "        \n",
    "        # EMLR\n",
    "        emlr_spec = np.exp(np.log(q_ens/p_ens).mean(dim='time')) \n",
    "        emlr.loc[dict(hour=f\"{h:02}\")] = freq_smooth2(1/q_lmean.wavelength,emlr_spec,winfac=winfac)\n",
    "\n",
    "        # Correlation (Spearman ranked)\n",
    "        #corr_spec = np.spearmanr(q_ens,p_ens,dim='time')\n",
    "        # Correlation (Pearsons applied to the log of PSD)\n",
    "        corr_spec = xr.corr(np.log(q_ens),np.log(p_ens),dim='time')\n",
    "        corr.loc[dict(hour=f\"{h:02}\")] = freq_smooth2(1/q_lmean.wavelength,corr_spec,winfac=winfac)\n",
    "        \n",
    "        # For relative entropy, there are a few steps... \n",
    "        #   first normalize by log mean across dates - what does this accomplish?\n",
    "        #   Puts all wavenumbers on a more equal footing\n",
    "        #   Also appears to ameliorate somewhat the result's dependency on which is p vs q\n",
    "        q_norm = q_ens/q_lmean.sel(hour=h)\n",
    "        p_norm = p_ens/q_lmean.sel(hour=h)\n",
    "        # Calculate the histograms across bins\n",
    "        #   Verified that shapes are maintained when we add 1 in each bin to ensure no +inf:\n",
    "        #   I think this is the way to go... any value would be arbitrary...\n",
    "        if len(sam_p.shape) == 2:\n",
    "            q_his1 = np.stack([np.histogram(q_norm[:,i],bins=bins)[0] for i in range(q_norm.shape[1])]) + 1\n",
    "            p_his1 = np.stack([np.histogram(p_norm[:,i],bins=bins)[0] for i in range(p_norm.shape[1])]) + 1\n",
    "        else: # Vertically varying fields\n",
    "            q_his1 = np.stack([[np.histogram(q_norm[:,j,i],bins=bins)[0] for i in range(q_norm.shape[2])] for j in range(q_norm.shape[1])]) + 1\n",
    "            p_his1 = np.stack([[np.histogram(p_norm[:,j,i],bins=bins)[0] for i in range(p_norm.shape[2])] for j in range(q_norm.shape[1])]) + 1 \n",
    "        # Convert to a probability distribution (sum = 1)\n",
    "        p_pd1 = p_his1/np.expand_dims(p_his1.sum(axis=-1),axis=-1)\n",
    "        q_pd1 = q_his1/np.expand_dims(q_his1.sum(axis=-1),axis=-1)\n",
    "        # Calculate relative entropy\n",
    "        re_spec = entropy(p_pd1,qk=q_pd1,axis=-1)\n",
    "        rex.loc[dict(hour=f\"{h:02}\")] = freq_smooth2(1/q_lmean.wavelength,re_spec,winfac=winfac)\n",
    "\n",
    "    if first: # Merge the DataArrays into one Dataset\n",
    "        #ds = rex.to_dataset().assign_attrs({'description':d_string})\n",
    "        ds = xr.merge([eml_het,eml_hom,lv_het,lv_hom,emlr,corr,rex]).assign_attrs({'description':d_string})\n",
    "        first = False\n",
    "    else:\n",
    "        ds[eml_het_name] = eml_het\n",
    "        ds[eml_hom_name] = eml_hom\n",
    "        ds[lv_het_name] = lv_het\n",
    "        ds[lv_hom_name] = lv_hom\n",
    "        ds[emlr_name] = emlr\n",
    "        ds[corr_name] = corr\n",
    "        ds[rex_name] = rex\n",
    "\n",
    "    print(\" \")\n",
    "    #sys.exit(0)\n",
    "    \n",
    "# For deflated NetCDF4 output\n",
    "deflate = dict(zlib=True, complevel=1)\n",
    "encoding = {var: deflate for var in ds.data_vars}\n",
    "\n",
    "ds.to_netcdf(f\"{ddir}fr2_stats2_PSD.nc4\",engine=\"netcdf4\",format=\"netCDF4\",encoding=encoding)\n",
    "\n",
    "print(\"***DONE***\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "A310",
   "language": "python",
   "name": "a310"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
